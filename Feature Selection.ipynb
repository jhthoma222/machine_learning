{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter \n",
    "code in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the \n",
    "accuracy. How many training points are there, according to the starter code?\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../tools/your_word_data.pkl\" \n",
    "authors_file = \"../tools/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9476678043230944\n",
      "Feature Ranking: \n",
      "1 feature no.33614 (0.764705882353)\n",
      "2 feature no.33201 (0.134028294862)\n",
      "3 feature no.19671 (0.0749500333111)\n",
      "4 feature no.24321 (0.0263157894737)\n",
      "5 feature no.12617 (0.0)\n",
      "6 feature no.12623 (0.0)\n",
      "7 feature no.12622 (0.0)\n",
      "8 feature no.12621 (0.0)\n",
      "9 feature no.12620 (0.0)\n",
      "10 feature no.12619 (0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'sshacklensf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take your (overfit) decision tree and use the feature_importances_ attribute to get a list of the relative importance of all \n",
    "the features being used. We suggest iterating through this list (it’s long, since this is text data) and only printing out the \n",
    "feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give \n",
    "an importance of far less than 0.01). What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../tools/your_word_data.pkl\" \n",
    "authors_file = \"../tools/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "\n",
    "# Find the top feature in the decision tree and its relative importance\n",
    "import numpy as np\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print 'Feature Ranking: '\n",
    "\n",
    "for i in range(10):\n",
    "    print \"{} feature no.{} ({})\".format(i+1,indices[i],importances[indices[i]])\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that \n",
    "you obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in \n",
    "the TfIdf by calling get_feature_names() on it; pull out the word that’s causing most of the discrimination of the decision \n",
    "tree. What is it? Does it make sense as a word that’s uniquely tied to either Chris Germany or Sara Shackleton, a signature of \n",
    "sorts?\n",
    "\"\"\"\n",
    "vectorizer.get_feature_names()[33614]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9476678043230944\n",
      "Feature Ranking: \n",
      "1 feature no.33614 (0.764705882353)\n",
      "2 feature no.33201 (0.134028294862)\n",
      "3 feature no.19671 (0.0749500333111)\n",
      "4 feature no.24321 (0.0263157894737)\n",
      "5 feature no.12617 (0.0)\n",
      "6 feature no.12623 (0.0)\n",
      "7 feature no.12622 (0.0)\n",
      "8 feature no.12621 (0.0)\n",
      "9 feature no.12620 (0.0)\n",
      "10 feature no.12619 (0.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py, \n",
    "and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, \n",
    "and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? \n",
    "(Define an outlier as a feature with importance >0.2, as before).\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../tools/your_word_data.pkl\" \n",
    "authors_file = \"../tools/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print 'Feature Ranking: '\n",
    "for i in range(10):\n",
    "    print \"{} feature no.{} ({})\".format(i+1,indices[i],importances[indices[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parseOutText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-6c1a9d95caac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m### use parseOutText to extract the text from the opened email\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0memail_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseOutText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m### use str.replace() to remove any instances of the words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parseOutText' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        #temp_counter += 1\n",
    "        #if temp_counter < 200:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            #print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            email_text = parseOutText(email)\n",
    "            \n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            list_rep  = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\"]\n",
    "            for e in list_rep:\n",
    "                email_text = email_text.replace(e,\"\")\n",
    "            \n",
    "            ### append the text to word_data\n",
    "            word_data.append(email_text)\n",
    "            \n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            if name == \"chris\":\n",
    "                from_data.append(1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "print word_data[152]\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "print len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python 2]",
   "language": "python",
   "name": "conda-env-Python_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
