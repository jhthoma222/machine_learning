{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 153.1 s\n",
      "881\n",
      "prediction time: 16.077 s\n",
      "0.9840728100113766\n"
     ]
    }
   ],
   "source": [
    "#SVM Mini Project\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel = 'linear', C=1)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print \"prediction time:\", round(time()-t0, 3), \"s\"\n",
    "print(clf.score(features_test, labels_test))\n",
    "\n",
    "#########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 0.089 s\n",
      "1046\n",
      "prediction time: 0.884 s\n",
      "0.8845278725824801\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "One way to speed up an algorithm is to train it on a smaller training dataset. The tradeoff is \n",
    "that the accuracy almost always goes down when you do this. Let’s explore this more concretely: add \n",
    "in the following two lines immediately before training your classifier.\n",
    "\n",
    "features_train = features_train[:len(features_train)/100]\n",
    "labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "These lines effectively slice the training dataset down to 1% of its original size, tossing out 99% \n",
    "of the training data. You can leave all other code unchanged. What’s the accuracy now?\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#These lines effectively slice the training dataset down to 1% of its original size, \n",
    "#tossing out 99% of the training data.\n",
    "\n",
    "features_train = features_train[:len(features_train)/100]\n",
    "labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "clf = SVC(kernel = 'linear', C=1)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print \"prediction time:\", round(time()-t0, 3), \"s\"\n",
    "print(clf.score(features_test, labels_test))\n",
    "\n",
    "#########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 0.099 s\n",
      "1540\n",
      "prediction time: 1.009 s\n",
      "0.6160409556313993\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keep the training set slice code from the last quiz, so that you are still training on only 1% of \n",
    "the full training set. Change the kernel of your SVM to “rbf”. What’s the accuracy now, with this \n",
    "more complex kernel?\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#These lines effectively slice the training dataset down to 1% of its original size, \n",
    "#tossing out 99% of the training data.\n",
    "\n",
    "features_train = features_train[:len(features_train)/100]\n",
    "labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "clf = SVC(kernel = 'rbf', C=1)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print \"prediction time:\", round(time()-t0, 3), \"s\"\n",
    "print(clf.score(features_test, labels_test))\n",
    "\n",
    "#########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 0.092 s\n",
      "1177\n",
      "prediction time: 0.949 s\n",
      "0.8213879408418657\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keep the training set size and rbf kernel from the last quiz, but try several values of C \n",
    "(say, 10.0, 100., 1000., and 10000.). Which one gives the best accuracy?\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#These lines effectively slice the training dataset down to 1% of its original size, \n",
    "#tossing out 99% of the training data.\n",
    "\n",
    "features_train = features_train[:len(features_train)/100]\n",
    "labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "clf = SVC(kernel = 'rbf', C=1000)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print \"prediction time:\", round(time()-t0, 3), \"s\"\n",
    "print(clf.score(features_test, labels_test))\n",
    "\n",
    "#########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 99.266 s\n",
      "877\n",
      "prediction time: 10.012 s\n",
      "0.9908987485779295\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that you’ve optimized C for the RBF kernel, go back to using the full training set. In general, \n",
    "having a larger training set will improve the performance of your algorithm, so (by tuning C and \n",
    "training on a large dataset) we should get a fairly optimized result. What is the accuracy of the \n",
    "optimized SVM?\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "clf = SVC(kernel = 'rbf', C=10000)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print \"prediction time:\", round(time()-t0, 3), \"s\"\n",
    "print(clf.score(features_test, labels_test))\n",
    "\n",
    "#########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 101.201 s\n",
      "After Classifying, the accuracy of SVM author identifier is:\n",
      "0.9908987485779295\n",
      "Prediction for element 10 of the test set? The 26th? The 50th?  1 0 1\n",
      "No of predicted Chris's emails are:  877\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What class does your SVM (0 or 1, corresponding to Sara and Chris respectively) predict for element\n",
    "10 of the test set? The 26th? The 50th? (Use the RBF kernel, C=10000, and 1% of the training set. \n",
    "Normally you'd get the best results using the full training set, but we found that using 1% sped up \n",
    "the computation considerably and did not change our results--so feel free to use that shortcut here.)\n",
    "\n",
    "And just to be clear, the data point numbers that we give here (10, 26, 50) assume a zero-indexed \n",
    "list. So the correct answer for element #100 would be found using something like answer=predictions\n",
    "[100]\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "clf = SVC(kernel = 'rbf', C=10000)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "print (\"After Classifying, the accuracy of SVM author identifier is:\")\n",
    "print accuracy_score(pred,labels_test)\n",
    "print \"Prediction for element 10 of the test set? The 26th? The 50th? \",pred[10],pred[26],pred[50]\n",
    "\n",
    "count = 0\n",
    "for ii in pred:\n",
    "    if ii == 1:\n",
    "        count += 1\n",
    "print \"No of predicted Chris's emails are: \",count\n",
    "\n",
    "\n",
    "#########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python 2]",
   "language": "python",
   "name": "conda-env-Python_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
