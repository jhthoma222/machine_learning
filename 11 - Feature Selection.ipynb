{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9476678043230944\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../test/your_word_data.pkl\" \n",
    "authors_file = \"../test/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\"))\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "print \"Accuracy:\", clf.score(features_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9476678043230944\n",
      "Feature # 33614\n",
      "Importance 0.7647058823529412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'sshacklensf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take your (overfit) decision tree and use the feature_importances_ attribute to get a list of the relative importance of all \n",
    "the features being used. We suggest iterating through this list (it’s long, since this is text data) and only printing out the \n",
    "feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give \n",
    "an importance of far less than 0.01). What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../test/your_word_data.pkl\" \n",
    "authors_file = \"../test/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\"))\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print \"Accuracy:\", clf.score(features_test, labels_test)\n",
    "\n",
    "\n",
    "# Find the top feature in the decision tree and its relative importance\n",
    "\n",
    "for index, feature in enumerate(clf.feature_importances_):\n",
    "    if feature>0.2:\n",
    "        print \"Feature #\", index        \n",
    "        print \"Importance\", feature\n",
    "        \n",
    "vectorizer.get_feature_names()[33614]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n",
      "tjonesnsf stephani and sam need nymex calendar \n",
      "17578\n",
      "38756\n",
      "stephanlonect\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py,\n",
    "and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py,\n",
    "and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? \n",
    "(Define an outlier as a feature with importance >0.2, as before).\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/text_learning/from_sara.txt\", \n",
    "                  \"r\")\n",
    "from_chris = open(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/text_learning/from_chris.txt\",\n",
    "                  \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        #temp_counter += 1\n",
    "        #if temp_counter:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            #print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            words = parseOutText(email)\n",
    "            \n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            list_rep  = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\"]\n",
    "            for e in list_rep:\n",
    "                words = words.replace(e,\"\")\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(words)\n",
    "            \n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            if name == \"chris\":\n",
    "                from_data.append(1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "print word_data[152]\n",
    "\n",
    "print len(word_data)\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "print len(vectorizer.get_feature_names())\n",
    "\n",
    "print vectorizer.get_feature_names()[34597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9505119453924915\n",
      "Feature # 14343\n",
      "Importance 0.6666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'cgermannsf'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py,\n",
    "and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, \n",
    "and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? \n",
    "(Define an outlier as a feature with importance >0.2, as before).\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../test/your_word_data.pkl\" \n",
    "authors_file = \"../test/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\"))\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print \"Accuracy:\", clf.score(features_test, labels_test)\n",
    "\n",
    "\n",
    "# Find the top feature in the decision tree and its relative importance\n",
    "\n",
    "for index, feature in enumerate(clf.feature_importances_):\n",
    "    if feature>0.2:\n",
    "        print \"Feature #\", index        \n",
    "        print \"Importance\", feature\n",
    "        \n",
    "vectorizer.get_feature_names()[14343]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n",
      "tjonesnsf stephani and sam need nymex calendar \n",
      "17578\n",
      "38755\n",
      "stephen\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Update vectorize_test.py one more time, and rerun. Then run find_signature.py again. Any other important features \n",
    "(importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that \n",
    "look like they legitimately come from the text of the messages?\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/tools\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/text_learning/from_sara.txt\", \n",
    "                  \"r\")\n",
    "from_chris = open(\"D:/Desktop/WGU Projects/data_analyst_nanodegree/machine_learning/ud120-projects-master/text_learning/from_chris.txt\",\n",
    "                  \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        #temp_counter += 1\n",
    "        #if temp_counter:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            #print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            words = parseOutText(email)\n",
    "            \n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            list_rep  = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\", \"cgermannsf\"]\n",
    "            for e in list_rep:\n",
    "                words = words.replace(e,\"\")\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(words)\n",
    "            \n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            if name == \"chris\":\n",
    "                from_data.append(1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "print word_data[152]\n",
    "\n",
    "print len(word_data)\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "print len(vectorizer.get_feature_names())\n",
    "\n",
    "print vectorizer.get_feature_names()[34597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8168373151308305\n",
      "Feature # 21323\n",
      "Importance 0.36363636363636365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'houectect'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../test/your_word_data.pkl\" \n",
    "authors_file = \"../test/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\"))\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print \"Accuracy:\", clf.score(features_test, labels_test)\n",
    "\n",
    "\n",
    "# Find the top feature in the decision tree and its relative importance\n",
    "\n",
    "for index, feature in enumerate(clf.feature_importances_):\n",
    "    if feature>0.2:\n",
    "        print \"Feature #\", index        \n",
    "        print \"Importance\", feature\n",
    "        \n",
    "vectorizer.get_feature_names()[21323]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python 2]",
   "language": "python",
   "name": "conda-env-Python_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
